{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "[' a']\n",
      "a\n",
      "[['on' 'ns' 's ' ' a' 'an' 'na' 'ar' 'rc' 'ch' 'hi' 'is']\n",
      " ['wh' 'he' 'en' 'n ' ' m' 'mi' 'il' 'li' 'it' 'ta' 'ar']\n",
      " ['ll' 'le' 'er' 'ri' 'ia' 'a ' ' a' 'ar' 'rc' 'ch' 'he']\n",
      " [' a' 'ab' 'bb' 'be' 'ey' 'ys' 's ' ' a' 'an' 'nd' 'd ']\n",
      " ['ma' 'ar' 'rr' 'ri' 'ie' 'ed' 'd ' ' u' 'ur' 'rr' 'ra']\n",
      " ['he' 'el' 'l ' ' a' 'an' 'nd' 'd ' ' r' 'ri' 'ic' 'ch']\n",
      " ['y ' ' a' 'an' 'nd' 'd ' ' l' 'li' 'it' 'tu' 'ur' 'rg']\n",
      " ['ay' 'y ' ' o' 'op' 'pe' 'en' 'ne' 'ed' 'd ' ' f' 'fo']\n",
      " ['ti' 'io' 'on' 'n ' ' f' 'fr' 'ro' 'om' 'm ' ' t' 'th']\n",
      " ['mi' 'ig' 'gr' 'ra' 'at' 'ti' 'io' 'on' 'n ' ' t' 'to']\n",
      " ['ne' 'ew' 'w ' ' y' 'yo' 'or' 'rk' 'k ' ' o' 'ot' 'th']\n",
      " ['he' 'e ' ' b' 'bo' 'oe' 'ei' 'in' 'ng' 'g ' ' s' 'se']\n",
      " ['e ' ' l' 'li' 'is' 'st' 'te' 'ed' 'd ' ' w' 'wi' 'it']\n",
      " ['eb' 'be' 'er' 'r ' ' h' 'ha' 'as' 's ' ' p' 'pr' 'ro']\n",
      " ['o ' ' b' 'be' 'e ' ' m' 'ma' 'ad' 'de' 'e ' ' t' 'to']\n",
      " ['ye' 'er' 'r ' ' w' 'wh' 'ho' 'o ' ' r' 're' 'ec' 'ce']\n",
      " ['or' 're' 'e ' ' s' 'si' 'ig' 'gn' 'ni' 'if' 'fi' 'ic']\n",
      " ['a ' ' f' 'fi' 'ie' 'er' 'rc' 'ce' 'e ' ' c' 'cr' 'ri']\n",
      " [' t' 'tw' 'wo' 'o ' ' s' 'si' 'ix' 'x ' ' e' 'ei' 'ig']\n",
      " ['ar' 'ri' 'is' 'st' 'to' 'ot' 'tl' 'le' 'e ' ' s' 's ']\n",
      " ['it' 'ty' 'y ' ' c' 'ca' 'an' 'n ' ' b' 'be' 'e ' ' l']\n",
      " [' a' 'an' 'nd' 'd ' ' i' 'in' 'nt' 'tr' 'ra' 'ac' 'ce']\n",
      " ['ti' 'io' 'on' 'n ' ' o' 'of' 'f ' ' t' 'th' 'he' 'e ']\n",
      " ['dy' 'y ' ' t' 'to' 'o ' ' p' 'pa' 'as' 'ss' 's ' ' h']\n",
      " ['f ' ' c' 'ce' 'er' 'rt' 'ta' 'ai' 'in' 'n ' ' d' 'dr']\n",
      " ['at' 't ' ' i' 'it' 't ' ' w' 'wi' 'il' 'll' 'l ' ' t']\n",
      " ['e ' ' c' 'co' 'on' 'nv' 'vi' 'in' 'nc' 'ce' 'e ' ' t']\n",
      " ['en' 'nt' 't ' ' t' 'to' 'ol' 'ld' 'd ' ' h' 'hi' 'im']\n",
      " ['am' 'mp' 'pa' 'ai' 'ig' 'gn' 'n ' ' a' 'an' 'nd' 'd ']\n",
      " ['rv' 've' 'er' 'r ' ' s' 'si' 'id' 'de' 'e ' ' s' 'st']\n",
      " ['io' 'ou' 'us' 's ' ' t' 'te' 'ex' 'xt' 'ts' 's ' ' s']\n",
      " ['o ' ' c' 'ca' 'ap' 'pi' 'it' 'ta' 'al' 'li' 'iz' 'ze']\n",
      " ['a ' ' d' 'du' 'up' 'pl' 'li' 'ic' 'ca' 'at' 'te' 'e ']\n",
      " ['gh' 'h ' ' a' 'an' 'nn' 'n ' ' e' 'es' 's ' ' d' 'd ']\n",
      " ['in' 'ne' 'e ' ' j' 'ja' 'an' 'nu' 'ua' 'ar' 'ry' 'y ']\n",
      " ['ro' 'os' 'ss' 's ' ' z' 'ze' 'er' 'ro' 'o ' ' t' 'th']\n",
      " ['ca' 'al' 'l ' ' t' 'th' 'he' 'eo' 'or' 'ri' 'ie' 'es']\n",
      " ['as' 'st' 't ' ' i' 'in' 'ns' 'st' 'ta' 'an' 'nc' 'ce']\n",
      " [' d' 'di' 'im' 'me' 'en' 'ns' 'si' 'io' 'on' 'na' 'al']\n",
      " ['mo' 'os' 'st' 't ' ' h' 'ho' 'ol' 'ly' 'y ' ' m' 'mo']\n",
      " ['t ' ' s' 's ' ' s' 'su' 'up' 'pp' 'po' 'or' 'rt' 't ']\n",
      " ['u ' ' i' 'is' 's ' ' s' 'st' 'ti' 'il' 'll' 'l ' ' d']\n",
      " ['e ' ' o' 'os' 'sc' 'ci' 'il' 'll' 'la' 'at' 'ti' 'in']\n",
      " ['o ' ' e' 'ei' 'ig' 'gh' 'ht' 't ' ' s' 'su' 'ub' 'bt']\n",
      " ['of' 'f ' ' i' 'it' 'ta' 'al' 'ly' 'y ' ' l' 'la' 'an']\n",
      " ['s ' ' t' 'th' 'he' 'e ' ' t' 'to' 'ow' 'we' 'er' 'r ']\n",
      " ['kl' 'la' 'ah' 'ho' 'om' 'ma' 'a ' ' p' 'pr' 're' 'es']\n",
      " ['er' 'rp' 'pr' 'ri' 'is' 'se' 'e ' ' l' 'li' 'in' 'nu']\n",
      " ['ws' 's ' ' b' 'be' 'ec' 'co' 'om' 'me' 'es' 's ' ' t']\n",
      " ['et' 't ' ' i' 'in' 'n ' ' a' 'a ' ' n' 'na' 'az' 'zi']\n",
      " ['th' 'he' 'e ' ' f' 'fa' 'ab' 'bi' 'ia' 'an' 'n ' ' s']\n",
      " ['et' 'tc' 'ch' 'hy' 'y ' ' t' 'to' 'o ' ' r' 're' 'el']\n",
      " [' s' 'sh' 'ha' 'ar' 'rm' 'ma' 'an' 'n ' ' n' 'ne' 'et']\n",
      " ['is' 'se' 'ed' 'd ' ' e' 'em' 'mp' 'pe' 'er' 'ro' 'or']\n",
      " ['ti' 'in' 'ng' 'g ' ' i' 'in' 'n ' ' p' 'po' 'ol' 'li']\n",
      " ['d ' ' n' 'ne' 'eo' 'o ' ' l' 'la' 'at' 'ti' 'in' 'n ']\n",
      " ['th' 'h ' ' r' 'ri' 'is' 'sk' 'ky' 'y ' ' r' 'ri' 'is']\n",
      " ['en' 'nc' 'cy' 'yc' 'cl' 'lo' 'op' 'pe' 'ed' 'di' 'ic']\n",
      " ['fe' 'en' 'ns' 'se' 'e ' ' t' 'th' 'he' 'e ' ' a' 'ai']\n",
      " ['du' 'ua' 'at' 'ti' 'in' 'ng' 'g ' ' f' 'fr' 'ro' 'om']\n",
      " ['tr' 're' 'ee' 'et' 't ' ' g' 'gr' 'ri' 'id' 'd ' ' c']\n",
      " ['at' 'ti' 'io' 'on' 'ns' 's ' ' m' 'mo' 'or' 're' 'e ']\n",
      " ['ap' 'pp' 'pe' 'ea' 'al' 'l ' ' o' 'of' 'f ' ' d' 'de']\n",
      " ['si' 'i ' ' h' 'ha' 'av' 've' 'e ' ' m' 'ma' 'ad' 'de']]\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "class BatchIdsGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size), dtype=np.int)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b] = char2id(self._text[self._cursor[b]])\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "class BatchBigramIdsGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    # The text is partitioned into batch_size ways\n",
    "    segment = self._text_size // batch_size\n",
    "    # The cursor is used to determine the next character to look at for a given batch\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "\n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, 2), dtype=np.int)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, 0] = char2id(self._text[self._cursor[b]])\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "      batch[b, 1] = char2id(self._text[self._cursor[b]])\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "def char_id_to_onehot(char_id):\n",
    "  enc = np.zeros(shape=(vocabulary_size), dtype=np.float)\n",
    "  enc[char_id] = 1.0\n",
    "  return enc\n",
    "\n",
    "def char_ids_to_onehots(char_ids):\n",
    "  return np.array([char_id_to_onehot(i) for i in char_ids])\n",
    "\n",
    "def char_ids_to_string(char_ids):\n",
    "  return ''.join([id2char(i) for i in char_ids])\n",
    "\n",
    "def onehot_to_char_id(onehot):\n",
    "  return np.argmax(onehot)\n",
    "\n",
    "def onehots_to_char_ids(onehots):\n",
    "  return np.array([onehot_to_char_id(oh) for oh in onehots])\n",
    "\n",
    "def bigram_batches_to_string(batches):\n",
    "  x = list()\n",
    "  for batch in batches:\n",
    "    x.append([char_ids_to_string(bigram) for bigram in batch])\n",
    "  return np.transpose(x)\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "train_id_batches = BatchIdsGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_id_batches = BatchIdsGenerator(valid_text, 1, 1)\n",
    "train_bigram_batches = BatchBigramIdsGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_bigram_batches = BatchBigramIdsGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(id2char(onehot_to_char_id(char_id_to_onehot(char2id('a')))))\n",
    "print(bigram_batches_to_string(train_bigram_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 256 #vocabulary_size\n",
    "use_dropout = False\n",
    "dropout_keep_prob = 0.75\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate bias.\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate bias.\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell bias.                             \n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate bias.\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "  # Combined input, forget, memory, and output weights\n",
    "  embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  iw = tf.Variable(tf.random_uniform([embedding_size, num_nodes * 4], -0.08, 0.08))\n",
    "  ow = tf.Variable(tf.random_uniform([num_nodes, num_nodes * 4], -0.08, 0.08))\n",
    "\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  bag_ids = [[i] * 2 for i in range(batch_size)]\n",
    "  bag_ids = tf.constant(np.reshape(bag_ids, -1), dtype=tf.int32)\n",
    "  sample_ids = [0, 0]\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state, dropout=False, sampling=False):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "\n",
    "    i = tf.reshape(i, [-1])\n",
    "    embed = tf.nn.embedding_lookup(embeddings, i)\n",
    "    if sampling:\n",
    "      embed = tf.segment_sum(embed, sample_ids)\n",
    "    else:\n",
    "      embed = tf.segment_sum(embed, bag_ids)\n",
    "    \n",
    "    if dropout:\n",
    "      embed = tf.nn.dropout(embed, dropout_keep_prob)\n",
    "    \n",
    "    iiw = tf.matmul(embed, iw)\n",
    "    iix = iiw[: , :num_nodes]\n",
    "    ifx = iiw[: , num_nodes:num_nodes*2]\n",
    "    icx = iiw[: , num_nodes*2:num_nodes*3]\n",
    "    iox = iiw[: , num_nodes*3:num_nodes*4]\n",
    "    \n",
    "    oow = tf.matmul(o, ow)\n",
    "    oim = oow[: , :num_nodes]\n",
    "    ofm = oow[: , num_nodes:num_nodes*2]\n",
    "    ocm = oow[: , num_nodes*2:num_nodes*3]\n",
    "    oom = oow[: , num_nodes*3:num_nodes*4]\n",
    "\n",
    "    input_gate = tf.sigmoid(iix + oim + ib)\n",
    "    forget_gate = tf.sigmoid(ifx + ofm + fb)\n",
    "    update = icx + ocm + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(iox + oom + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "  \n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    tensor = tf.placeholder(tf.int32, shape=(batch_size, 2))\n",
    "    train_data.append(tensor)\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "\n",
    "  label_mask = tf.constant([[False, True]] * batch_size)\n",
    "  # Remapping back into one-hot encoding\n",
    "  identity = tf.constant(np.identity(vocabulary_size, dtype=np.float32))\n",
    "  train_labels = [tf.nn.embedding_lookup(identity, tf.boolean_mask(tl, label_mask)) for tl in train_data[1:]]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state, dropout=use_dropout)\n",
    "    if use_dropout:\n",
    "      output = tf.nn.dropout(output, dropout_keep_prob)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.matmul(tf.concat(0, outputs), w) + b\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(5.0, global_step, 5000, 0.5, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1,2])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state, sampling=True)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.matmul(sample_output, w) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.293407 learning rate: 5.000000\n",
      "Minibatch perplexity: 26.93\n",
      "================================================================================\n",
      "yqyalg ylhhvvyrcwtlhsjdzj fxrstonox  op efpmej snkjskes a lacltcehsrw mntlygfofz\n",
      "msfpe rnnm itowogcucoyolwnbglnxyti n mor qseofrz ne ne otz hoeqenj keovmsndfbgen\n",
      "kswjqxnsahhwpqv  bulayegsepdfoedqyzbg faee dgpijfketgaqiszltnaebeztpm eeesnquawo\n",
      "xleinbmvx oehsoysdsvn a cshaqvyfb atxm ljrj aeddakaefltvehwwewlhpqdhohkhesiagint\n",
      "yxoo tyl vdkeissvmmgzyuvwnmkwyiwvn  qof ooxyn  wmo axluh  a  soyacretc yyb oxtio\n",
      "================================================================================\n",
      "Validation set perplexity: 19.20\n",
      "Average loss at step 200: 2.235547 learning rate: 5.000000\n",
      "Minibatch perplexity: 7.71\n",
      "Validation set perplexity: 8.50\n",
      "Average loss at step 400: 1.981974 learning rate: 5.000000\n",
      "Minibatch perplexity: 7.08\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 600: 1.916781 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.81\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 800: 1.850908 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 1000: 1.795504 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 6.16\n",
      "Average loss at step 1200: 1.781621 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 1400: 1.778067 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 1600: 1.744914 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.68\n",
      "Average loss at step 1800: 1.738904 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 2000: 1.725768 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.41\n",
      "================================================================================\n",
      "we of or peernes woblation yurlel ibea was as not doymtion caer sup eap from rai\n",
      "l oversions and sassen and smespyred with bottile e fbum mediticoguic of duch in\n",
      "khame stence of witring taces of has posersent one even two zero fin of this was\n",
      "kk is desco as mount one zero zero zero seven four the nod not a five forme natu\n",
      "k or or wroce and during timemabypic don the by graj in one nine most commonst a\n",
      "================================================================================\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 2200: 1.718218 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 2400: 1.703647 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2600: 1.681951 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2800: 1.689944 learning rate: 5.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3000: 1.688369 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 3200: 1.693495 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 3400: 1.681245 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3600: 1.683761 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3800: 1.659117 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4000: 1.667411 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.72\n",
      "================================================================================\n",
      " vcetazinian rook glod been angeles unier dater one and b untert also being two \n",
      "thperte mainadia and the power prine of the georst homard theoled their sittator\n",
      "p how two boot britcet nowar of the one ojes wrhene but genalainent perfedrencle\n",
      "sking tundera ontimited berning of the opent and hordming cof thesred accompired\n",
      "isple bett these in s the flim from includitco rahinch mnirts acth toracy s the \n",
      "================================================================================\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 4200: 1.653638 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 4400: 1.653122 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 4600: 1.645873 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4800: 1.627970 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 5000: 1.630298 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 5200: 1.623339 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5400: 1.584379 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5600: 1.594526 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5800: 1.603948 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 6000: 1.623330 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.39\n",
      "================================================================================\n",
      "lraben feation and one two zero three two eight zero sm clod of dinglandsel at t\n",
      "rn swing deopacets and the neads six six two seek continess his nations one nine\n",
      "ties will ramaries there playar in and the other indistic to vergoan morg amenn \n",
      "eof nealliet minishe again succouncous rendistry but the chapwudpent fancess eur\n",
      "vaers facordic nemturing russion i not germ of jakeing europendue by a typla s i\n",
      "================================================================================\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 6200: 1.606565 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6400: 1.589973 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6600: 1.577409 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6800: 1.598613 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 7000: 1.576020 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 7200: 1.612921 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 7400: 1.610590 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 7600: 1.611028 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 7800: 1.599554 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 8000: 1.628445 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.89\n",
      "================================================================================\n",
      "mslum marrist is one five five throut meter docsuad the mempt it her not to fhut\n",
      "yosm provwels of hoolilitition wemplish killby reporta restroly spreen of the or\n",
      " is on operately for howing three one nine six no s batting that jare reases tha\n",
      "ndosed that and the was enformanness approxites were rived theire and the his in\n",
      "cotionalism from the c parts an over backing the ter peninung after t attation a\n",
      "================================================================================\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 8200: 1.594751 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 8400: 1.594175 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 8600: 1.595577 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 8800: 1.588083 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 9000: 1.590671 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 9200: 1.582416 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 9400: 1.574917 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 9600: 1.603730 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 9800: 1.596290 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 10000: 1.602293 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.57\n",
      "================================================================================\n",
      "nying primu was ass the bir four the searhent actps four alonies of the also mea\n",
      "oquency htate brodmon aby utberx per animate equalir the pelliuse that in has be\n",
      "xv mamour surms to in essetine one three eight th unettaching in the aestry tim \n",
      "bx his andred two zero eight prime and moveted circum as battabel active rultrum\n",
      "sk public desia borject the reason one nive zero four a musclating wholehans bei\n",
      "================================================================================\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 10200: 1.569134 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 10400: 1.593358 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 10600: 1.564245 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 10800: 1.570727 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 11000: 1.551186 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 11200: 1.584339 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 11400: 1.572892 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 11600: 1.544717 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 11800: 1.559139 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 12000: 1.568835 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "ryriate denominal consteptudent enfulata wilest ideam bare such about economic f\n",
      "dauled vehicse judion in the alway rouclets begyposed the faction and generaves \n",
      "tnew drawgest are information are non relation suefformation of ornity ramiaring\n",
      "rpaterminesported mass then pespas ailer government and materican then rejectuul\n",
      "fative to instrrovio com the hom com long im low horted only the six piece humed\n",
      "================================================================================\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 12200: 1.558951 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 12400: 1.562701 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.22\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 12600: 1.558392 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.23\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 12800: 1.566204 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 13000: 1.566720 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 13200: 1.570279 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 13400: 1.563119 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 13600: 1.557837 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 13800: 1.555875 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 14000: 1.563899 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.42\n",
      "================================================================================\n",
      "dzed from onlist the meature addreded by inficele of the only the sistendent noz\n",
      "hsetantal in the new ford a ger to kingded with padmagds a some is one three are\n",
      "mmig whiles the incluessi follow age dietrals impacion level of cause pepreffern\n",
      "tc nucless to origing of seconle play numberly unites wisl people adomy leaving \n",
      "qeu the pai italians j the own dold raphime that the by expliginal malts grallin\n",
      "================================================================================\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 14200: 1.580493 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 14400: 1.585751 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 14600: 1.596414 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 14800: 1.574106 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 15000: 1.552594 learning rate: 0.625000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 15200: 1.564386 learning rate: 0.625000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 15400: 1.574929 learning rate: 0.625000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 15600: 1.550694 learning rate: 0.625000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 15800: 1.573118 learning rate: 0.625000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 16000: 1.574632 learning rate: 0.625000\n",
      "Minibatch perplexity: 5.14\n",
      "================================================================================\n",
      "pding enoust women people albert of not half was clautes anthren with three zero\n",
      " is legan where distriates of the tarmed by raboo report d evst if formant to di\n",
      "tq one sums in aeriages feer anneigh ejampion animess and dubly continuelly firs\n",
      "zy a metrowing outps cd one nine nine nine sed one three two can huns have alsed\n",
      "pjysical dewiding activily on we xviell for pnizaphy buckets all human diread be\n",
      "================================================================================\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 16200: 1.549749 learning rate: 0.625000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 16400: 1.566133 learning rate: 0.625000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 16600: 1.582135 learning rate: 0.625000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 16800: 1.578180 learning rate: 0.625000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 17000: 1.567451 learning rate: 0.625000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 17200: 1.588962 learning rate: 0.625000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 17400: 1.574567 learning rate: 0.625000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 17600: 1.575012 learning rate: 0.625000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 17800: 1.585447 learning rate: 0.625000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 18000: 1.556726 learning rate: 0.625000\n",
      "Minibatch perplexity: 4.92\n",
      "================================================================================\n",
      "zchannots the madr anning that the recorge pubublisk sund dictia sechrison produ\n",
      "mandatic to dealistractions when in soin of usigal separt yauscyhnasz to evites \n",
      "lbited chrisized and cr anting are lassanes on inform zone through end sergarch \n",
      "knut continiaths tudent term was context heart four three  the regarite regions \n",
      "hvying circlethman cased the lefcient of the escelleased by set also membry is t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 18200: 1.582325 learning rate: 0.625000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 18400: 1.575377 learning rate: 0.625000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 18600: 1.566674 learning rate: 0.625000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 18800: 1.553543 learning rate: 0.625000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 19000: 1.564326 learning rate: 0.625000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 19200: 1.533515 learning rate: 0.625000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 19400: 1.529151 learning rate: 0.625000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 19600: 1.527466 learning rate: 0.625000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 19800: 1.516340 learning rate: 0.625000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 20000: 1.519110 learning rate: 0.312500\n",
      "Minibatch perplexity: 4.61\n",
      "================================================================================\n",
      "ficatia low frelling come unwerens instring that the region attender chrimbr wha\n",
      "pilal in presultania comple ballez nobaths other place late ii land level which \n",
      "aft provides into to foothmit with s yuugs one five anal companial had it iplain\n",
      "kutlry de five six zero is but the syzroughter shint bractsis in fighter as cent\n",
      "yz the followed under all lasts policy and this largenral not three one two coun\n",
      "================================================================================\n",
      "Validation set perplexity: 4.28\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "summary_frequency = 200\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_bigram_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = [np.ma.array(b, mask=([[1,0]] * len(b))).compressed() for b in list(batches)[1:]]\n",
    "      labels = [char_ids_to_onehots(label) for label in labels]\n",
    "      labels = np.concatenate(labels)\n",
    "      print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = [sample(random_distribution()), sample(random_distribution())]\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(78):\n",
    "            input_data = [onehot_to_char_id(f) for f in feed]\n",
    "            prediction = sample_prediction.eval({sample_input: [input_data]})\n",
    "            feed = [feed[-1], sample(prediction)]\n",
    "            sentence += characters(feed[-1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_bigram_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, char_ids_to_onehots([b[1][0][1]]))\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
