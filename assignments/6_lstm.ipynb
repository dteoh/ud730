{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "[' a']\n",
      "a\n",
      "[['on' 'ns' 's ' ' a' 'an' 'na' 'ar' 'rc' 'ch' 'hi' 'is']\n",
      " ['wh' 'he' 'en' 'n ' ' m' 'mi' 'il' 'li' 'it' 'ta' 'ar']\n",
      " ['ll' 'le' 'er' 'ri' 'ia' 'a ' ' a' 'ar' 'rc' 'ch' 'he']\n",
      " [' a' 'ab' 'bb' 'be' 'ey' 'ys' 's ' ' a' 'an' 'nd' 'd ']\n",
      " ['ma' 'ar' 'rr' 'ri' 'ie' 'ed' 'd ' ' u' 'ur' 'rr' 'ra']\n",
      " ['he' 'el' 'l ' ' a' 'an' 'nd' 'd ' ' r' 'ri' 'ic' 'ch']\n",
      " ['y ' ' a' 'an' 'nd' 'd ' ' l' 'li' 'it' 'tu' 'ur' 'rg']\n",
      " ['ay' 'y ' ' o' 'op' 'pe' 'en' 'ne' 'ed' 'd ' ' f' 'fo']\n",
      " ['ti' 'io' 'on' 'n ' ' f' 'fr' 'ro' 'om' 'm ' ' t' 'th']\n",
      " ['mi' 'ig' 'gr' 'ra' 'at' 'ti' 'io' 'on' 'n ' ' t' 'to']\n",
      " ['ne' 'ew' 'w ' ' y' 'yo' 'or' 'rk' 'k ' ' o' 'ot' 'th']\n",
      " ['he' 'e ' ' b' 'bo' 'oe' 'ei' 'in' 'ng' 'g ' ' s' 'se']\n",
      " ['e ' ' l' 'li' 'is' 'st' 'te' 'ed' 'd ' ' w' 'wi' 'it']\n",
      " ['eb' 'be' 'er' 'r ' ' h' 'ha' 'as' 's ' ' p' 'pr' 'ro']\n",
      " ['o ' ' b' 'be' 'e ' ' m' 'ma' 'ad' 'de' 'e ' ' t' 'to']\n",
      " ['ye' 'er' 'r ' ' w' 'wh' 'ho' 'o ' ' r' 're' 'ec' 'ce']\n",
      " ['or' 're' 'e ' ' s' 'si' 'ig' 'gn' 'ni' 'if' 'fi' 'ic']\n",
      " ['a ' ' f' 'fi' 'ie' 'er' 'rc' 'ce' 'e ' ' c' 'cr' 'ri']\n",
      " [' t' 'tw' 'wo' 'o ' ' s' 'si' 'ix' 'x ' ' e' 'ei' 'ig']\n",
      " ['ar' 'ri' 'is' 'st' 'to' 'ot' 'tl' 'le' 'e ' ' s' 's ']\n",
      " ['it' 'ty' 'y ' ' c' 'ca' 'an' 'n ' ' b' 'be' 'e ' ' l']\n",
      " [' a' 'an' 'nd' 'd ' ' i' 'in' 'nt' 'tr' 'ra' 'ac' 'ce']\n",
      " ['ti' 'io' 'on' 'n ' ' o' 'of' 'f ' ' t' 'th' 'he' 'e ']\n",
      " ['dy' 'y ' ' t' 'to' 'o ' ' p' 'pa' 'as' 'ss' 's ' ' h']\n",
      " ['f ' ' c' 'ce' 'er' 'rt' 'ta' 'ai' 'in' 'n ' ' d' 'dr']\n",
      " ['at' 't ' ' i' 'it' 't ' ' w' 'wi' 'il' 'll' 'l ' ' t']\n",
      " ['e ' ' c' 'co' 'on' 'nv' 'vi' 'in' 'nc' 'ce' 'e ' ' t']\n",
      " ['en' 'nt' 't ' ' t' 'to' 'ol' 'ld' 'd ' ' h' 'hi' 'im']\n",
      " ['am' 'mp' 'pa' 'ai' 'ig' 'gn' 'n ' ' a' 'an' 'nd' 'd ']\n",
      " ['rv' 've' 'er' 'r ' ' s' 'si' 'id' 'de' 'e ' ' s' 'st']\n",
      " ['io' 'ou' 'us' 's ' ' t' 'te' 'ex' 'xt' 'ts' 's ' ' s']\n",
      " ['o ' ' c' 'ca' 'ap' 'pi' 'it' 'ta' 'al' 'li' 'iz' 'ze']\n",
      " ['a ' ' d' 'du' 'up' 'pl' 'li' 'ic' 'ca' 'at' 'te' 'e ']\n",
      " ['gh' 'h ' ' a' 'an' 'nn' 'n ' ' e' 'es' 's ' ' d' 'd ']\n",
      " ['in' 'ne' 'e ' ' j' 'ja' 'an' 'nu' 'ua' 'ar' 'ry' 'y ']\n",
      " ['ro' 'os' 'ss' 's ' ' z' 'ze' 'er' 'ro' 'o ' ' t' 'th']\n",
      " ['ca' 'al' 'l ' ' t' 'th' 'he' 'eo' 'or' 'ri' 'ie' 'es']\n",
      " ['as' 'st' 't ' ' i' 'in' 'ns' 'st' 'ta' 'an' 'nc' 'ce']\n",
      " [' d' 'di' 'im' 'me' 'en' 'ns' 'si' 'io' 'on' 'na' 'al']\n",
      " ['mo' 'os' 'st' 't ' ' h' 'ho' 'ol' 'ly' 'y ' ' m' 'mo']\n",
      " ['t ' ' s' 's ' ' s' 'su' 'up' 'pp' 'po' 'or' 'rt' 't ']\n",
      " ['u ' ' i' 'is' 's ' ' s' 'st' 'ti' 'il' 'll' 'l ' ' d']\n",
      " ['e ' ' o' 'os' 'sc' 'ci' 'il' 'll' 'la' 'at' 'ti' 'in']\n",
      " ['o ' ' e' 'ei' 'ig' 'gh' 'ht' 't ' ' s' 'su' 'ub' 'bt']\n",
      " ['of' 'f ' ' i' 'it' 'ta' 'al' 'ly' 'y ' ' l' 'la' 'an']\n",
      " ['s ' ' t' 'th' 'he' 'e ' ' t' 'to' 'ow' 'we' 'er' 'r ']\n",
      " ['kl' 'la' 'ah' 'ho' 'om' 'ma' 'a ' ' p' 'pr' 're' 'es']\n",
      " ['er' 'rp' 'pr' 'ri' 'is' 'se' 'e ' ' l' 'li' 'in' 'nu']\n",
      " ['ws' 's ' ' b' 'be' 'ec' 'co' 'om' 'me' 'es' 's ' ' t']\n",
      " ['et' 't ' ' i' 'in' 'n ' ' a' 'a ' ' n' 'na' 'az' 'zi']\n",
      " ['th' 'he' 'e ' ' f' 'fa' 'ab' 'bi' 'ia' 'an' 'n ' ' s']\n",
      " ['et' 'tc' 'ch' 'hy' 'y ' ' t' 'to' 'o ' ' r' 're' 'el']\n",
      " [' s' 'sh' 'ha' 'ar' 'rm' 'ma' 'an' 'n ' ' n' 'ne' 'et']\n",
      " ['is' 'se' 'ed' 'd ' ' e' 'em' 'mp' 'pe' 'er' 'ro' 'or']\n",
      " ['ti' 'in' 'ng' 'g ' ' i' 'in' 'n ' ' p' 'po' 'ol' 'li']\n",
      " ['d ' ' n' 'ne' 'eo' 'o ' ' l' 'la' 'at' 'ti' 'in' 'n ']\n",
      " ['th' 'h ' ' r' 'ri' 'is' 'sk' 'ky' 'y ' ' r' 'ri' 'is']\n",
      " ['en' 'nc' 'cy' 'yc' 'cl' 'lo' 'op' 'pe' 'ed' 'di' 'ic']\n",
      " ['fe' 'en' 'ns' 'se' 'e ' ' t' 'th' 'he' 'e ' ' a' 'ai']\n",
      " ['du' 'ua' 'at' 'ti' 'in' 'ng' 'g ' ' f' 'fr' 'ro' 'om']\n",
      " ['tr' 're' 'ee' 'et' 't ' ' g' 'gr' 'ri' 'id' 'd ' ' c']\n",
      " ['at' 'ti' 'io' 'on' 'ns' 's ' ' m' 'mo' 'or' 're' 'e ']\n",
      " ['ap' 'pp' 'pe' 'ea' 'al' 'l ' ' o' 'of' 'f ' ' d' 'de']\n",
      " ['si' 'i ' ' h' 'ha' 'av' 've' 'e ' ' m' 'ma' 'ad' 'de']]\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "class BatchIdsGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size), dtype=np.int)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b] = char2id(self._text[self._cursor[b]])\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "class BatchBigramIdsGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    # The text is partitioned into batch_size ways\n",
    "    segment = self._text_size // batch_size\n",
    "    # The cursor is used to determine the next character to look at for a given batch\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "\n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, 2), dtype=np.int)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, 0] = char2id(self._text[self._cursor[b]])\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "      batch[b, 1] = char2id(self._text[self._cursor[b]])\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "def char_id_to_onehot(char_id):\n",
    "  enc = np.zeros(shape=(vocabulary_size), dtype=np.float)\n",
    "  enc[char_id] = 1.0\n",
    "  return enc\n",
    "\n",
    "def char_ids_to_onehots(char_ids):\n",
    "  return np.array([char_id_to_onehot(i) for i in char_ids])\n",
    "\n",
    "def char_ids_to_string(char_ids):\n",
    "  return ''.join([id2char(i) for i in char_ids])\n",
    "\n",
    "def onehot_to_char_id(onehot):\n",
    "  return np.argmax(onehot)\n",
    "\n",
    "def onehots_to_char_ids(onehots):\n",
    "  return np.array([onehot_to_char_id(oh) for oh in onehots])\n",
    "\n",
    "def bigram_batches_to_string(batches):\n",
    "  x = list()\n",
    "  for batch in batches:\n",
    "    x.append([char_ids_to_string(bigram) for bigram in batch])\n",
    "  return np.transpose(x)\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "train_id_batches = BatchIdsGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_id_batches = BatchIdsGenerator(valid_text, 1, 1)\n",
    "train_bigram_batches = BatchBigramIdsGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_bigram_batches = BatchBigramIdsGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(id2char(onehot_to_char_id(char_id_to_onehot(char2id('a')))))\n",
    "print(bigram_batches_to_string(train_bigram_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = vocabulary_size\n",
    "use_dropout = False\n",
    "dropout_keep_prob = 0.75\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate bias.\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate bias.\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell bias.                             \n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate bias.\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "  # Combined input, forget, memory, and output weights\n",
    "  embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  iw = tf.Variable(tf.truncated_normal([embedding_size, num_nodes * 4], -0.1, 0.1))\n",
    "  ow = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  bag_ids = [[i] * 2 for i in range(batch_size)]\n",
    "  bag_ids = tf.constant(np.reshape(bag_ids, -1), dtype=tf.int32)\n",
    "  sample_ids = [0, 0]\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state, dropout=False, sampling=False):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "\n",
    "    i = tf.reshape(i, [-1])\n",
    "    embed = tf.nn.embedding_lookup(embeddings, i)\n",
    "    if sampling:\n",
    "      embed = tf.segment_sum(embed, sample_ids)\n",
    "    else:\n",
    "      embed = tf.segment_sum(embed, bag_ids)\n",
    "    \n",
    "    if dropout:\n",
    "      embed = tf.nn.dropout(embed, dropout_keep_prob)\n",
    "    \n",
    "    iiw = tf.matmul(embed, iw)\n",
    "    iix = iiw[: , :num_nodes]\n",
    "    ifx = iiw[: , num_nodes:num_nodes*2]\n",
    "    icx = iiw[: , num_nodes*2:num_nodes*3]\n",
    "    iox = iiw[: , num_nodes*3:num_nodes*4]\n",
    "    \n",
    "    oow = tf.matmul(o, ow)\n",
    "    oim = oow[: , :num_nodes]\n",
    "    ofm = oow[: , num_nodes:num_nodes*2]\n",
    "    ocm = oow[: , num_nodes*2:num_nodes*3]\n",
    "    oom = oow[: , num_nodes*3:num_nodes*4]\n",
    "\n",
    "    input_gate = tf.sigmoid(iix + oim + ib)\n",
    "    forget_gate = tf.sigmoid(ifx + ofm + fb)\n",
    "    update = icx + ocm + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(iox + oom + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "  \n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    tensor = tf.placeholder(tf.int32, shape=(batch_size, 2))\n",
    "    train_data.append(tensor)\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "\n",
    "  label_mask = tf.constant([[False, True]] * batch_size)\n",
    "  identity = tf.constant(np.identity(embedding_size, dtype=np.float32))\n",
    "  train_labels = [tf.nn.embedding_lookup(identity, tf.boolean_mask(tl, label_mask)) for tl in train_data[1:]]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state, dropout=use_dropout)\n",
    "    if use_dropout:\n",
    "      output = tf.nn.dropout(output, dropout_keep_prob)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.matmul(tf.concat(0, outputs), w) + b\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1,2])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state, sampling=True)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.matmul(sample_output, w) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.302485 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.18\n",
      "================================================================================\n",
      "po  xqjbishf hem ndeb bg lgpto fyyerbg udh cehstk niehmoyfpmesb  oq jr  emvpirgm\n",
      "xn lgajagqlanqiyzprznhkxevtxasjwqzgpkmoadad meh eu hfsup av ti oytahe itw eme  f\n",
      "rzsv lnuasfkons jv mmrae nqwsyeae  i ch t shnpgdgeecplaeqdn v kbqie  eti j vyzhh\n",
      "itew ktmj  t ieedidvvzsf x iee  qnei va  evvhvucppfwwhrk fgds  muvtvq  t vaog t \n",
      "zvtn  ece nuchavi olg qr eet  ev  ked  ibx drnirpzesuiigtw gyifpeneztc e rhtikcr\n",
      "================================================================================\n",
      "Validation set perplexity: 19.36\n",
      "Average loss at step 100: 2.370210 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.73\n",
      "Validation set perplexity: 8.89\n",
      "Average loss at step 200: 2.051667 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 300: 1.922120 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 400: 1.891108 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 500: 1.839022 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 600: 1.836178 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 700: 1.829284 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.93\n",
      "Average loss at step 800: 1.756249 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 900: 1.737403 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 1000: 1.763308 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "================================================================================\n",
      "rficels and them taction are one zero the gest monow buy whun promusp neelf proc\n",
      "cyallingaul as celosed wins cars rese explegt sort har an ond new forp whites ex\n",
      "ive satbroval repure of their a for endon cendreiral publices catuate one the us\n",
      "d pleviry in ercial expaed are greasua were the pelation feaded gest several a s\n",
      "gwuly sassit trlicms shomsowratianiyss and the descing in becaminic provity sham\n",
      "================================================================================\n",
      "Validation set perplexity: 5.87\n",
      "Average loss at step 1100: 1.774041 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 5.80\n",
      "Average loss at step 1200: 1.743142 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.69\n",
      "Average loss at step 1300: 1.748794 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 5.68\n",
      "Average loss at step 1400: 1.765683 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 1500: 1.748879 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 1600: 1.687766 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 1700: 1.708228 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1800: 1.728492 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1900: 1.710665 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 2000: 1.695232 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.59\n",
      "================================================================================\n",
      "cientity it pabuggaarhym lealitainst nugidave only and productogensions arage un\n",
      "vqick partely election malthym katemted of that ans re through again prokies joh\n",
      "uatic magcessivers two zero five one five surduates and ordived four was distric\n",
      "spolited of theobelitafto rivearian newst dos nottogen fmearing renquate vustsbo\n",
      "yi and poflinan liguutar iquiessional b vrystane the machery liagrand whotewems \n",
      "================================================================================\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 2100: 1.690394 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 2200: 1.708423 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 2300: 1.727488 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 2400: 1.708079 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 2500: 1.695500 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 2600: 1.729633 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2700: 1.700324 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 2800: 1.685453 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2900: 1.687813 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 3000: 1.727925 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "================================================================================\n",
      "aln didgrticion mals articians toopes laethral signlean a have supp of diarissin\n",
      "wvioced of was zero zero zero eight zero nine hid lotheophation fady we monadity\n",
      " esmited neting govell words of ramishing otherall officernetic siber and into i\n",
      "jhicy armine gre a in walleguuash world by pyht mant peopear have arwossion the \n",
      "qre stanges nandlan run the time her infall refl wit of f came with paline marhy\n",
      "================================================================================\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 3100: 1.711192 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 3200: 1.715020 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 3300: 1.707677 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 3400: 1.687657 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 3500: 1.684108 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 3600: 1.702401 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 3700: 1.690553 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 3800: 1.687720 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 3900: 1.692871 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 4000: 1.677775 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "================================================================================\n",
      "xbis prevenilicy thy simber stillony this firchics indes and thin regiolozing do\n",
      "kk runglishi in reailided strol of the in commember wave besinting of nub trate \n",
      "vved by that with that this two unchies areing of as the hoolk lair to beside of\n",
      "ajny strek of only in of which system of baskold to setue wipogo evente defnrad \n",
      "gbeunnt due all chemigroed computrizating production one publing homes large thi\n",
      "================================================================================\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 4100: 1.670806 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 4200: 1.690958 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 4300: 1.687132 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 4400: 1.683049 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 4500: 1.667684 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 4600: 1.677805 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 4700: 1.685430 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 4800: 1.662566 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 4900: 1.648135 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 5000: 1.702022 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "================================================================================\n",
      "bu to lond or referses oldear are the lin be nalonies case a gibland a lay on un\n",
      "pea the mast in there case nation wriber replacate one nine very of the gler of \n",
      "obutgens cape munk intered free out ocnened curie united only inses one in olifi\n",
      "yi enversibe court hil is been housion hit new s and includination one nine thre\n",
      "bx belies freen by as in from prie which grejors the sound feaco the if fared mo\n",
      "================================================================================\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 5100: 1.656592 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 5200: 1.614034 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 5300: 1.598176 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 5400: 1.587388 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 5500: 1.597912 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 5600: 1.621039 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 5700: 1.651726 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 5800: 1.629388 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 5900: 1.614972 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 6000: 1.593531 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.89\n",
      "================================================================================\n",
      "ps prres dand moral the precix articly s consurie rivers on its the etrachy of s\n",
      "k american wardlaguinsts bect were servally is the survessic equrtachen disted a\n",
      "xise any garding war of lawfefilr an eth euginal classily with history figersert\n",
      "jcase in at all m takes one nine seven zero he by a name bairlings about out lea\n",
      "tban the be the dilf ballumit of universion and ha zero fwarent and paviirts a n\n",
      "================================================================================\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 6100: 1.589140 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 6200: 1.624934 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 6300: 1.578039 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 6400: 1.584097 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 6500: 1.583797 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 6600: 1.576204 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.14\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 6700: 1.596860 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 6800: 1.600926 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 6900: 1.640598 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 7000: 1.636238 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "================================================================================\n",
      "qman duriates one one nine three nine two two ore newway jamiem one nine nine ze\n",
      "aviters neople over to and rois prly is a germany rain kensiwineth horsell physi\n",
      "ck for bet in this straifiess colleving can rievs emersorshiis be would was gen \n",
      "lcdes jen partines factors and setterningby gui strencity is one six three seven\n",
      " vaitional and one nine eight nine nine airstomon was terl mologsions nuck right\n",
      "================================================================================\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 7100: 1.643907 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 7200: 1.597534 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 7300: 1.608960 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 7400: 1.569670 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 7500: 1.579424 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 7600: 1.571763 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 7700: 1.571598 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 7800: 1.609026 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 7900: 1.592427 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 8000: 1.579135 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "================================================================================\n",
      "to courting bonce agains wirdh est in a phility two zero zero zero coonge the in\n",
      "ecoliged lockazst resliams them sabot the controphors zerober although call tu s\n",
      "hfwold to there the pereduct pheri of s noths in day telors for they woildrill b\n",
      "gy s but she was moresalled played indiania ships that his waighloweve th harr n\n",
      "dd domass the american clarbent songs sclewants from which did the first discoot\n",
      "================================================================================\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 8100: 1.567167 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 8200: 1.587586 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 8300: 1.579222 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 8400: 1.579811 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 8500: 1.580019 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 8600: 1.553522 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 8700: 1.588852 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 8800: 1.580159 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 8900: 1.578943 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 9000: 1.567061 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "================================================================================\n",
      "ambal include fisther has wide h afquene theihobicar mesperizing descule was tha\n",
      "sk renauphlery species by gaminity the scolparasisipe off the players beloid als\n",
      "gd plane in move one malta miling to has the allemy m that it producan parts gen\n",
      " ahaca this corper reened be a righting and and have to a beavy cheneted at the \n",
      "hhips kepell form rustimanism peopued is sharcult and the were the rathers entor\n",
      "================================================================================\n",
      "Validation set perplexity: 4.43\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_bigram_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = [np.ma.array(b, mask=([[1,0]] * len(b))).compressed() for b in list(batches)[1:]]\n",
    "      labels = [char_ids_to_onehots(label) for label in labels]\n",
    "      labels = np.concatenate(labels)\n",
    "      print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = [sample(random_distribution()), sample(random_distribution())]\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(78):\n",
    "            input_data = [onehot_to_char_id(f) for f in feed]\n",
    "            prediction = sample_prediction.eval({sample_input: [input_data]})\n",
    "            feed = [feed[-1], sample(prediction)]\n",
    "            sentence += characters(feed[-1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_bigram_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, char_ids_to_onehots([b[1][0][1]]))\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
