{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "[' a']\n",
      "a\n",
      "[['on' 'ns' 's ' ' a' 'an' 'na' 'ar' 'rc' 'ch' 'hi' 'is']\n",
      " ['wh' 'he' 'en' 'n ' ' m' 'mi' 'il' 'li' 'it' 'ta' 'ar']\n",
      " ['ll' 'le' 'er' 'ri' 'ia' 'a ' ' a' 'ar' 'rc' 'ch' 'he']\n",
      " [' a' 'ab' 'bb' 'be' 'ey' 'ys' 's ' ' a' 'an' 'nd' 'd ']\n",
      " ['ma' 'ar' 'rr' 'ri' 'ie' 'ed' 'd ' ' u' 'ur' 'rr' 'ra']\n",
      " ['he' 'el' 'l ' ' a' 'an' 'nd' 'd ' ' r' 'ri' 'ic' 'ch']\n",
      " ['y ' ' a' 'an' 'nd' 'd ' ' l' 'li' 'it' 'tu' 'ur' 'rg']\n",
      " ['ay' 'y ' ' o' 'op' 'pe' 'en' 'ne' 'ed' 'd ' ' f' 'fo']\n",
      " ['ti' 'io' 'on' 'n ' ' f' 'fr' 'ro' 'om' 'm ' ' t' 'th']\n",
      " ['mi' 'ig' 'gr' 'ra' 'at' 'ti' 'io' 'on' 'n ' ' t' 'to']\n",
      " ['ne' 'ew' 'w ' ' y' 'yo' 'or' 'rk' 'k ' ' o' 'ot' 'th']\n",
      " ['he' 'e ' ' b' 'bo' 'oe' 'ei' 'in' 'ng' 'g ' ' s' 'se']\n",
      " ['e ' ' l' 'li' 'is' 'st' 'te' 'ed' 'd ' ' w' 'wi' 'it']\n",
      " ['eb' 'be' 'er' 'r ' ' h' 'ha' 'as' 's ' ' p' 'pr' 'ro']\n",
      " ['o ' ' b' 'be' 'e ' ' m' 'ma' 'ad' 'de' 'e ' ' t' 'to']\n",
      " ['ye' 'er' 'r ' ' w' 'wh' 'ho' 'o ' ' r' 're' 'ec' 'ce']\n",
      " ['or' 're' 'e ' ' s' 'si' 'ig' 'gn' 'ni' 'if' 'fi' 'ic']\n",
      " ['a ' ' f' 'fi' 'ie' 'er' 'rc' 'ce' 'e ' ' c' 'cr' 'ri']\n",
      " [' t' 'tw' 'wo' 'o ' ' s' 'si' 'ix' 'x ' ' e' 'ei' 'ig']\n",
      " ['ar' 'ri' 'is' 'st' 'to' 'ot' 'tl' 'le' 'e ' ' s' 's ']\n",
      " ['it' 'ty' 'y ' ' c' 'ca' 'an' 'n ' ' b' 'be' 'e ' ' l']\n",
      " [' a' 'an' 'nd' 'd ' ' i' 'in' 'nt' 'tr' 'ra' 'ac' 'ce']\n",
      " ['ti' 'io' 'on' 'n ' ' o' 'of' 'f ' ' t' 'th' 'he' 'e ']\n",
      " ['dy' 'y ' ' t' 'to' 'o ' ' p' 'pa' 'as' 'ss' 's ' ' h']\n",
      " ['f ' ' c' 'ce' 'er' 'rt' 'ta' 'ai' 'in' 'n ' ' d' 'dr']\n",
      " ['at' 't ' ' i' 'it' 't ' ' w' 'wi' 'il' 'll' 'l ' ' t']\n",
      " ['e ' ' c' 'co' 'on' 'nv' 'vi' 'in' 'nc' 'ce' 'e ' ' t']\n",
      " ['en' 'nt' 't ' ' t' 'to' 'ol' 'ld' 'd ' ' h' 'hi' 'im']\n",
      " ['am' 'mp' 'pa' 'ai' 'ig' 'gn' 'n ' ' a' 'an' 'nd' 'd ']\n",
      " ['rv' 've' 'er' 'r ' ' s' 'si' 'id' 'de' 'e ' ' s' 'st']\n",
      " ['io' 'ou' 'us' 's ' ' t' 'te' 'ex' 'xt' 'ts' 's ' ' s']\n",
      " ['o ' ' c' 'ca' 'ap' 'pi' 'it' 'ta' 'al' 'li' 'iz' 'ze']\n",
      " ['a ' ' d' 'du' 'up' 'pl' 'li' 'ic' 'ca' 'at' 'te' 'e ']\n",
      " ['gh' 'h ' ' a' 'an' 'nn' 'n ' ' e' 'es' 's ' ' d' 'd ']\n",
      " ['in' 'ne' 'e ' ' j' 'ja' 'an' 'nu' 'ua' 'ar' 'ry' 'y ']\n",
      " ['ro' 'os' 'ss' 's ' ' z' 'ze' 'er' 'ro' 'o ' ' t' 'th']\n",
      " ['ca' 'al' 'l ' ' t' 'th' 'he' 'eo' 'or' 'ri' 'ie' 'es']\n",
      " ['as' 'st' 't ' ' i' 'in' 'ns' 'st' 'ta' 'an' 'nc' 'ce']\n",
      " [' d' 'di' 'im' 'me' 'en' 'ns' 'si' 'io' 'on' 'na' 'al']\n",
      " ['mo' 'os' 'st' 't ' ' h' 'ho' 'ol' 'ly' 'y ' ' m' 'mo']\n",
      " ['t ' ' s' 's ' ' s' 'su' 'up' 'pp' 'po' 'or' 'rt' 't ']\n",
      " ['u ' ' i' 'is' 's ' ' s' 'st' 'ti' 'il' 'll' 'l ' ' d']\n",
      " ['e ' ' o' 'os' 'sc' 'ci' 'il' 'll' 'la' 'at' 'ti' 'in']\n",
      " ['o ' ' e' 'ei' 'ig' 'gh' 'ht' 't ' ' s' 'su' 'ub' 'bt']\n",
      " ['of' 'f ' ' i' 'it' 'ta' 'al' 'ly' 'y ' ' l' 'la' 'an']\n",
      " ['s ' ' t' 'th' 'he' 'e ' ' t' 'to' 'ow' 'we' 'er' 'r ']\n",
      " ['kl' 'la' 'ah' 'ho' 'om' 'ma' 'a ' ' p' 'pr' 're' 'es']\n",
      " ['er' 'rp' 'pr' 'ri' 'is' 'se' 'e ' ' l' 'li' 'in' 'nu']\n",
      " ['ws' 's ' ' b' 'be' 'ec' 'co' 'om' 'me' 'es' 's ' ' t']\n",
      " ['et' 't ' ' i' 'in' 'n ' ' a' 'a ' ' n' 'na' 'az' 'zi']\n",
      " ['th' 'he' 'e ' ' f' 'fa' 'ab' 'bi' 'ia' 'an' 'n ' ' s']\n",
      " ['et' 'tc' 'ch' 'hy' 'y ' ' t' 'to' 'o ' ' r' 're' 'el']\n",
      " [' s' 'sh' 'ha' 'ar' 'rm' 'ma' 'an' 'n ' ' n' 'ne' 'et']\n",
      " ['is' 'se' 'ed' 'd ' ' e' 'em' 'mp' 'pe' 'er' 'ro' 'or']\n",
      " ['ti' 'in' 'ng' 'g ' ' i' 'in' 'n ' ' p' 'po' 'ol' 'li']\n",
      " ['d ' ' n' 'ne' 'eo' 'o ' ' l' 'la' 'at' 'ti' 'in' 'n ']\n",
      " ['th' 'h ' ' r' 'ri' 'is' 'sk' 'ky' 'y ' ' r' 'ri' 'is']\n",
      " ['en' 'nc' 'cy' 'yc' 'cl' 'lo' 'op' 'pe' 'ed' 'di' 'ic']\n",
      " ['fe' 'en' 'ns' 'se' 'e ' ' t' 'th' 'he' 'e ' ' a' 'ai']\n",
      " ['du' 'ua' 'at' 'ti' 'in' 'ng' 'g ' ' f' 'fr' 'ro' 'om']\n",
      " ['tr' 're' 'ee' 'et' 't ' ' g' 'gr' 'ri' 'id' 'd ' ' c']\n",
      " ['at' 'ti' 'io' 'on' 'ns' 's ' ' m' 'mo' 'or' 're' 'e ']\n",
      " ['ap' 'pp' 'pe' 'ea' 'al' 'l ' ' o' 'of' 'f ' ' d' 'de']\n",
      " ['si' 'i ' ' h' 'ha' 'av' 've' 'e ' ' m' 'ma' 'ad' 'de']]\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "class BatchIdsGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size), dtype=np.int)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b] = char2id(self._text[self._cursor[b]])\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "class BatchBigramIdsGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    # The text is partitioned into batch_size ways\n",
    "    segment = self._text_size // batch_size\n",
    "    # The cursor is used to determine the next character to look at for a given batch\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "\n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, 2), dtype=np.int)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, 0] = char2id(self._text[self._cursor[b]])\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "      batch[b, 1] = char2id(self._text[self._cursor[b]])\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "def char_id_to_onehot(char_id):\n",
    "  enc = np.zeros(shape=(vocabulary_size), dtype=np.float)\n",
    "  enc[char_id] = 1.0\n",
    "  return enc\n",
    "\n",
    "def char_ids_to_onehots(char_ids):\n",
    "  return np.array([char_id_to_onehot(i) for i in char_ids])\n",
    "\n",
    "def char_ids_to_string(char_ids):\n",
    "  return ''.join([id2char(i) for i in char_ids])\n",
    "\n",
    "def onehot_to_char_id(onehot):\n",
    "  return np.argmax(onehot)\n",
    "\n",
    "def onehots_to_char_ids(onehots):\n",
    "  return np.array([onehot_to_char_id(oh) for oh in onehots])\n",
    "\n",
    "def bigram_batches_to_string(batches):\n",
    "  x = list()\n",
    "  for batch in batches:\n",
    "    x.append([char_ids_to_string(bigram) for bigram in batch])\n",
    "  return np.transpose(x)\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "train_id_batches = BatchIdsGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_id_batches = BatchIdsGenerator(valid_text, 1, 1)\n",
    "train_bigram_batches = BatchBigramIdsGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_bigram_batches = BatchBigramIdsGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(id2char(onehot_to_char_id(char_id_to_onehot(char2id('a')))))\n",
    "print(bigram_batches_to_string(train_bigram_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 256 #vocabulary_size\n",
    "use_dropout = True\n",
    "dropout_keep_prob = 0.75\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate bias.\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate bias.\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell bias.                             \n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate bias.\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "  # Combined input, forget, memory, and output weights\n",
    "  embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  iw = tf.Variable(tf.truncated_normal([embedding_size, num_nodes * 4], -0.1, 0.1))\n",
    "  ow = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  bag_ids = [[i] * 2 for i in range(batch_size)]\n",
    "  bag_ids = tf.constant(np.reshape(bag_ids, -1), dtype=tf.int32)\n",
    "  sample_ids = [0, 0]\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state, dropout=False, sampling=False):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "\n",
    "    i = tf.reshape(i, [-1])\n",
    "    embed = tf.nn.embedding_lookup(embeddings, i)\n",
    "    if sampling:\n",
    "      embed = tf.segment_sum(embed, sample_ids)\n",
    "    else:\n",
    "      embed = tf.segment_sum(embed, bag_ids)\n",
    "    \n",
    "    if dropout:\n",
    "      embed = tf.nn.dropout(embed, dropout_keep_prob)\n",
    "    \n",
    "    iiw = tf.matmul(embed, iw)\n",
    "    iix = iiw[: , :num_nodes]\n",
    "    ifx = iiw[: , num_nodes:num_nodes*2]\n",
    "    icx = iiw[: , num_nodes*2:num_nodes*3]\n",
    "    iox = iiw[: , num_nodes*3:num_nodes*4]\n",
    "    \n",
    "    oow = tf.matmul(o, ow)\n",
    "    oim = oow[: , :num_nodes]\n",
    "    ofm = oow[: , num_nodes:num_nodes*2]\n",
    "    ocm = oow[: , num_nodes*2:num_nodes*3]\n",
    "    oom = oow[: , num_nodes*3:num_nodes*4]\n",
    "\n",
    "    input_gate = tf.sigmoid(iix + oim + ib)\n",
    "    forget_gate = tf.sigmoid(ifx + ofm + fb)\n",
    "    update = icx + ocm + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(iox + oom + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "  \n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    tensor = tf.placeholder(tf.int32, shape=(batch_size, 2))\n",
    "    train_data.append(tensor)\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "\n",
    "  label_mask = tf.constant([[False, True]] * batch_size)\n",
    "  # Remapping back into one-hot encoding\n",
    "  identity = tf.constant(np.identity(vocabulary_size, dtype=np.float32))\n",
    "  train_labels = [tf.nn.embedding_lookup(identity, tf.boolean_mask(tl, label_mask)) for tl in train_data[1:]]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state, dropout=use_dropout)\n",
    "    if use_dropout:\n",
    "      output = tf.nn.dropout(output, dropout_keep_prob)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.matmul(tf.concat(0, outputs), w) + b\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1,2])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state, sampling=True)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.matmul(sample_output, w) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.327329 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.86\n",
      "================================================================================\n",
      "iqe  n  bukubr ons umethem  whodp ewx l e ple khut psrwmhm ejfumin drrkqfckfeuke\n",
      "oyinf  fsnq lfodooiubeec j   vtqrjrib eahnfueorrkrssw fvky  nig  utrjm a ozoocdq\n",
      "libu aas cdbmupfjy rf  rg  rke uo m rybdtures h  emsa owrckvo o eomx ztihzturpoj\n",
      "mm fkeeycposu rjjsj xevfezkgaeac a kijv iisns nag la aidea em wdbt eft u kwm hhf\n",
      "l kbjb navwd vqrne bexib g sekoyrkorvre ts f il xaa k ld n i  rtu s iejirksrym a\n",
      "================================================================================\n",
      "Validation set perplexity: 19.98\n",
      "Average loss at step 100: 2.509568 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.06\n",
      "Validation set perplexity: 10.44\n",
      "Average loss at step 200: 2.287361 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.23\n",
      "Validation set perplexity: 9.31\n",
      "Average loss at step 300: 2.189945 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.41\n",
      "Validation set perplexity: 9.07\n",
      "Average loss at step 400: 2.175462 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.30\n",
      "Validation set perplexity: 8.40\n",
      "Average loss at step 500: 2.151447 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.57\n",
      "Validation set perplexity: 8.17\n",
      "Average loss at step 600: 2.134677 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.90\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 700: 2.094507 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.01\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 800: 2.066421 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.90\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 900: 2.073547 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.16\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 1000: 2.068672 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.10\n",
      "================================================================================\n",
      "jmopts cat of the hacn d intates two heroying alge of the cales or nine five por\n",
      "jvesity atasting powpe malour three pary zero qepronce wadof the centalens in rr\n",
      "ivat is for chosel londsqeus reasasfre the wol aniamecorrition have fiy two sump\n",
      "xnia lar muxe thy indisyt sebel wpwuat by metpes sting bater reations derakle na\n",
      "fjoricaser on somnaty gsen offen cing mation a semertions cblypiroyc are to six \n",
      "================================================================================\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 1100: 2.078972 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.50\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 1200: 2.069240 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.62\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 1300: 2.039164 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.51\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 1400: 2.027197 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.87\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 1500: 2.023810 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.94\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 1600: 2.036632 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.56\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 1700: 1.982499 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.62\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 1800: 1.997562 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 1900: 2.004833 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.45\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 2000: 1.976373 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.59\n",
      "================================================================================\n",
      " ckon in it in nise thahule a strea as egins is with is central is while is faed\n",
      "atde adms uchins a ugh meutican bewlofetic the  subses euge ar horde a sijeu foo\n",
      "zten of nroms thaj ilms at by imand kext holstdund and the f is the eight the ma\n",
      " be siled of pilly game lary it be jors ming perfor beeth the latcal sibr a was \n",
      " witkical to by was socirveld ytwo cbont leas laped dentall six sborther found s\n",
      "================================================================================\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 2100: 1.989753 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.41\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 2200: 1.995953 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.12\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 2300: 1.995742 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.46\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 2400: 2.017615 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.44\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 2500: 2.014384 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.12\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 2600: 1.992905 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.21\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 2700: 1.985691 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.09\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 2800: 1.969192 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.96\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 2900: 1.942866 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.80\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 3000: 1.960647 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.48\n",
      "================================================================================\n",
      " peoblive frach mouts by from size nomtinantal many ernoply peall derjoosal the \n",
      "ihdons most of the demia adert his ariles from found natainal beasions ko repush\n",
      "n saintion the consal to the he mower von funda barr five scontimov leiget leis \n",
      "xkell also barita lallequots anfay afs crezration the coesingted is provate or a\n",
      "jru in cors ons ecateman as kett with imparund dering fant fhe dibaly and dhe of\n",
      "================================================================================\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 3100: 1.950304 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.61\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 3200: 1.978620 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.99\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 3300: 1.978590 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.90\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 3400: 1.973209 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.82\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 3500: 1.946895 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 3600: 1.957195 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.79\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 3700: 1.957849 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.46\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 3800: 1.968411 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.17\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 3900: 1.961146 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.25\n",
      "Validation set perplexity: 6.17\n",
      "Average loss at step 4000: 1.945009 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.58\n",
      "================================================================================\n",
      "dgs comuniedial pritor to scics qialeitcuch udght to dewsicce potic sysary achis\n",
      "ex back was would the were musons raand gence geninge plocestic bit forma the wa\n",
      "yj eingt botre dions one the ling mout of grianet onlissicte biidugation the on \n",
      "wkilly a dearkatietic ay both federd it oqueen wave ling stantrls punis pasuist \n",
      "zst the ebaed the for are and conisn ections and the frevout a sher merpo and la\n",
      "================================================================================\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 4100: 1.949647 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.36\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 4200: 1.958518 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 4300: 1.954594 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.97\n",
      "Validation set perplexity: 6.17\n",
      "Average loss at step 4400: 1.947189 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.62\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 4500: 1.955139 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.74\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 4600: 1.948239 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.52\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 4700: 1.946699 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 6.11\n",
      "Average loss at step 4800: 1.974683 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 4900: 1.954368 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.88\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 5000: 1.953178 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.97\n",
      "================================================================================\n",
      "bfer condux de ebly angs proped of jatiang excewjeht well centreusnally to by la\n",
      "iwfiel of tim incian the on entaller of rougtling forour a demary linslave de of\n",
      "rged and betwer one five kis zero srilo pratore corvent boodient parten boi the \n",
      "qluainited part sbattal inted the eareon placy haved eriecian of one five d of h\n",
      "aulk sterepear retween the rek in soraga lin in cloterression of generims mulso \n",
      "================================================================================\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 5100: 1.930741 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.29\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 5200: 1.931580 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.62\n",
      "Validation set perplexity: 5.90\n",
      "Average loss at step 5300: 1.926240 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.34\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 5400: 1.910959 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 5500: 1.932902 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.98\n",
      "Validation set perplexity: 5.79\n",
      "Average loss at step 5600: 1.921575 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.64\n",
      "Validation set perplexity: 5.80\n",
      "Average loss at step 5700: 1.907668 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.76\n",
      "Validation set perplexity: 5.79\n",
      "Average loss at step 5800: 1.904085 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 5900: 1.897140 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.31\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 6000: 1.935974 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.56\n",
      "================================================================================\n",
      "itanzegrgy matet the promenacy theors pirboot sever fames ireng hik emble mode a\n",
      "by five vansing bur seim he anlyonal devibe time six zero five zero ss which the\n",
      "wnhping after ose from to elaworns fro tems and neosy ganges besain paviotion an\n",
      "lfiim the fecdim verds the for throgahhision micric most coplamadence numsahu a \n",
      "qjurnin land stated hapy a and mulintated irimihc ekinz onf alloss the destand b\n",
      "================================================================================\n",
      "Validation set perplexity: 5.82\n",
      "Average loss at step 6100: 1.907899 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.33\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 6200: 1.912994 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 6300: 1.914060 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.16\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 6400: 1.908658 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.77\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 6500: 1.887511 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 6600: 1.905015 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 5.78\n",
      "Average loss at step 6700: 1.889343 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 6800: 1.885332 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.69\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 6900: 1.876072 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 7000: 1.896147 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.67\n",
      "================================================================================\n",
      "opistst and on the blic eight sout to a sewra deciginikoleis speatital the comme\n",
      "vxer united be of mur in a dafquall ecpt and they in mabbody cwe theirse one nin\n",
      "jvaing is mumpliy bactor tearkge ant from is is its to one corvitsed a nine from\n",
      "oame gealtant swrated the commors hilard is it no mast se achus decent ordes ryi\n",
      "xt ementer crectong scover by knok occmerranmer the lacecourding as one baist se\n",
      "================================================================================\n",
      "Validation set perplexity: 5.78\n",
      "Average loss at step 7100: 1.908071 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 7200: 1.882099 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.52\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 7300: 1.855966 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 7400: 1.891045 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 7500: 1.881225 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.89\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 7600: 1.882059 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 5.69\n",
      "Average loss at step 7700: 1.896448 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 7800: 1.888141 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 5.69\n",
      "Average loss at step 7900: 1.917133 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.09\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 8000: 1.918387 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.40\n",
      "================================================================================\n",
      "nices judist and regpulenil which selopes whitesired lome dicied conture singing\n",
      "ipy tequentian porcital the communsed the the gruemal provisided baation constri\n",
      "pved for the edick by bever on sacce varcommattions coinferbalican trind the bni\n",
      "yqhuafns foun desproted boy a will and in  new six was intruff cent eightts been\n",
      "aqeucg muliye three and his highred peroing at lmation etworkys paorter a to in \n",
      "================================================================================\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 8100: 1.890641 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.25\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 8200: 1.902626 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 8300: 1.888421 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 8400: 1.885576 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.61\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 8500: 1.872641 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 8600: 1.860128 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 5.68\n",
      "Average loss at step 8700: 1.903551 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.95\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 8800: 1.892497 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 8900: 1.885135 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 9000: 1.849187 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.26\n",
      "================================================================================\n",
      "c holsated she hillsoro three foul feasta premally werenticasses the crecar lite\n",
      "wcewn ishamresulpsed party donal apre sive for in nine two mitene jethile develi\n",
      "dj to five ritime gerecist miuseds home doints weres an ipsumerit ether the rual\n",
      "opn of reasi iceadenti exten green destant his or the i glaons dons and a to tha\n",
      "xpera two five nine five dageral dreasurring many hams of litest lacky fine priv\n",
      "================================================================================\n",
      "Validation set perplexity: 5.71\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_bigram_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = [np.ma.array(b, mask=([[1,0]] * len(b))).compressed() for b in list(batches)[1:]]\n",
    "      labels = [char_ids_to_onehots(label) for label in labels]\n",
    "      labels = np.concatenate(labels)\n",
    "      print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = [sample(random_distribution()), sample(random_distribution())]\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(78):\n",
    "            input_data = [onehot_to_char_id(f) for f in feed]\n",
    "            prediction = sample_prediction.eval({sample_input: [input_data]})\n",
    "            feed = [feed[-1], sample(prediction)]\n",
    "            sentence += characters(feed[-1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_bigram_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, char_ids_to_onehots([b[1][0][1]]))\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
